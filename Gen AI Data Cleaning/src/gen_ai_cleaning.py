# -*- coding: utf-8 -*-
"""Gen AI Cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rumdYLBY9sWOq_LmpCxBXT3YfZXtT2KX
"""

# importing every important libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# loading the dataset in a dataframe
df = pd.read_csv("diabetes.csv")
df.info()

def handle_outliers(df):

    # checking for outliers in numeric columns with IQR
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        # I put a tolerance value, so not every outlier getting cleared
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # replace outliers with NaNs
        df[col] = df[col].apply(
            lambda x: x if (x >= lower_bound and x <= upper_bound) else None
        )

        # boxplot for visualisation
        plt.figure(figsize=(6,4))
        plt.boxplot(df[col].dropna())
        plt.title(f'Boxplot for {col} after cleaning')
        plt.show()

    return df

def invalid_zeros(df):
  cols_with_invalid_zeros = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]

# 0-values marking as NaNs
  df[cols_with_invalid_zeros] = df[cols_with_invalid_zeros].replace(0.0, np.nan)

# filling NaNs with the mean
  for col in cols_with_invalid_zeros:
    df[col] = df[col].fillna(df[col].mean())

  return df

def clean_missing_values(df):

    # counting missing values
    missing = df.isnull().sum()
    total_missing = missing.sum()
    if total_missing > 0:
        print("Missing values found:")
        print(missing[missing > 0])
    else:
        print("No missing values found.")

    # cleaning the missing values in numeric columns with mean
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        if df[col].isnull().sum() > 0:
            # mean_value is the mean of the mentioned column
            mean_value = df[col].mean()
            df[col].fillna(mean_value)
            print(f"Column '{col}' cleaned with mean = {mean_value:.2f}")

    # cleaning the missing values in categorical columns with the most used value
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            # mode_value is the most used value in the mentioned column
            mode_value = df[col].mode()[0]
            df[col].fillna(mode_value)
            print(f"Column '{col}' cleaned with most used value = '{mode_value}'")

    return df

def check_and_fix_dtypes(df):

    print("Before cleaning:")
    print(df.dtypes)

    # checking for wrong data types
    for col in df.columns:
        # if a column is has object as dtype it tries to change to numeric
        # 'raise' causes a error if this is not possible and the data type
        # remains an object
        if df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col], errors='raise')
                print(f"Column '{col}' from object → numeric converted.")
            except:
                try:
                    # if the dtype is an object and no numeric column it also
                    # tries to change the object to datetime
                    # if the column is a categorical column 'raise' prevent a
                    # change and the column remains an object
                    df[col] = pd.to_datetime(df[col], errors='raise')
                    print(f"Column '{col}' from object → datetime converted.")
                except:
                    pass

    print("\nAfter cleaning:")
    print(df.dtypes)

    return df

def check_and_remove_duplicates(df):

    # counts the number of duplicates
    duplicates_count = df.duplicated().sum()
    print(f"Found duplicates: {duplicates_count}")

    # drops the duplicates and prints the change from before and after cleaning
    df = df.drop_duplicates()

    print(f"After Cleaning: {len(df)} rows (before {len(df)})")

    return df

def scale_and_encode(df):

    # scaling numeric columns with the StandardScaler because this is a dataset
    # for a Machine Learning Model
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    scaler = StandardScaler()
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    print(f"Scaled numeric columns: {list(numeric_cols)}")

    # transform categorical columns with the OneHotEncoder in numbers to make
    # the columns useable for Machine Learning
    categorical_cols = df.select_dtypes(include=['object']).columns
    if len(categorical_cols) > 0:
        encoder = OneHotEncoder(sparse=False, drop='first')  # drop first, to prevent of redundancy
        ohe_data = encoder.fit_transform(df[categorical_cols])
        ohe_df = pd.DataFrame(ohe_data, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)
        df = df.drop(categorical_cols, axis=1)
        df = pd.concat([df, ohe_df], axis=1)
        print(f"One-Hot-Encoded Columns: {list(categorical_cols)}")
    else:
        print("No Columns for One-Hot-Encoding found.")


    return df

def save_cleaned_data(df,filename):
  # changing the dataframe in a csv
  df.to_csv(filename, index=False)
  print(f"CSV-file '{filename}' is saved")

  return df

df = handle_outliers(df)
df = clean_missing_values(df)
df = check_and_fix_dtypes(df)
df = check_and_remove_duplicates(df)
df = scale_and_encode(df)
df = invalid_zeros(df)
filename = "Cleaned_data"
df = save_cleaned_data(df,filename)